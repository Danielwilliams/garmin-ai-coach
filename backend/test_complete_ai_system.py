"""Complete AI System Validation Test.

This comprehensive test validates the entire AI analysis pipeline from start to finish,
ensuring all components work together correctly and produce expected outputs.
"""

import asyncio
import logging
import uuid
from datetime import datetime
from typing import Dict, Any, Optional

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MockDatabase:
    """Mock database for testing without actual database connections."""
    
    def __init__(self):
        self.analyses = {}
        self.results = {}
    
    async def execute(self, query):
        """Mock execute method."""
        return MockResult()
    
    async def commit(self):
        """Mock commit method."""
        pass
    
    async def rollback(self):
        """Mock rollback method."""
        pass
    
    def add(self, obj):
        """Mock add method."""
        pass


class MockResult:
    """Mock database result."""
    
    def scalar_one_or_none(self):
        return None


class CompleteSystemValidator:
    """Validator for the complete AI analysis system."""
    
    def __init__(self):
        self.test_results = {}
        self.mock_db = MockDatabase()
    
    async def test_state_management_system(self) -> bool:
        """Test the TrainingAnalysisState management system."""
        
        logger.info("Testing state management system...")
        
        try:
            from app.services.ai.langgraph.state.training_analysis_state import (
                create_initial_state,
                update_progress,
                add_token_usage,
                TokenUsage,
                validate_state_integrity
            )
            
            # Create initial state
            analysis_config = {
                "analysis_type": "full_analysis",
                "ai_mode": "development",
                "activities_days": 21,
                "metrics_days": 56,
                "enable_plotting": False
            }
            
            state = create_initial_state(
                analysis_id=str(uuid.uuid4()),
                user_id=str(uuid.uuid4()),
                training_config_id=str(uuid.uuid4()),
                analysis_config=analysis_config
            )
            
            # Test state structure
            required_fields = [
                "analysis_id", "user_id", "training_config_id",
                "workflow_id", "analysis_type", "current_step"
            ]
            
            for field in required_fields:
                assert field in state, f"Required field {field} missing from state"
            
            # Test progress updates
            state = update_progress(state, "test_step", 50.0)
            assert state["current_step"] == "test_step"
            assert state["progress"].progress_percentage == 50.0
            
            # Test token usage tracking
            token_usage = TokenUsage(
                total_tokens=100,
                prompt_tokens=50,
                completion_tokens=50,
                estimated_cost=0.002,
                model_used="test-model",
                provider="test"
            )
            
            state = add_token_usage(state, "test_agent", token_usage)
            assert "test_agent" in state["token_usage"]
            assert state["total_tokens"] == 100
            assert state["total_cost"] == 0.002
            
            # Test state validation
            validation_issues = validate_state_integrity(state)
            assert len(validation_issues) == 0, f"State validation failed: {validation_issues}"
            
            logger.info("‚úÖ State management system working correctly")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå State management test failed: {e}")
            return False
    
    async def test_model_configuration_system(self) -> bool:
        """Test the AI model configuration system."""
        
        logger.info("Testing model configuration system...")
        
        try:
            from app.services.ai.model_config import (
                ModelManager,
                AIMode,
                MODE_ASSIGNMENTS
            )
            
            # Test model assignments exist
            for mode in AIMode:
                assert mode in MODE_ASSIGNMENTS, f"Mode {mode} missing from assignments"
                config = MODE_ASSIGNMENTS[mode]
                
                required_keys = ["primary_model", "fallback_model", "cost_effective_model"]
                for key in required_keys:
                    assert key in config, f"Key {key} missing from mode {mode} config"
            
            # Test ModelManager initialization
            manager = ModelManager(AIMode.DEVELOPMENT)
            
            # Test configuration retrieval
            config = manager.get_model_config(AIMode.DEVELOPMENT)
            assert config is not None
            assert "primary_model" in config
            
            logger.info("‚úÖ Model configuration system working correctly")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Model configuration test failed: {e}")
            return False
    
    async def test_garmin_data_extraction(self) -> bool:
        """Test the Garmin data extraction pipeline."""
        
        logger.info("Testing Garmin data extraction...")
        
        try:
            from app.services.garmin.data_extractor import (
                extract_garmin_data,
                ExtractionConfig
            )
            from app.services.garmin.models import GarminData
            
            # Test extraction configuration
            config = ExtractionConfig(
                activities_days=21,
                metrics_days=56,
                include_detailed_activities=True
            )
            
            assert config.activities_days == 21
            assert config.metrics_days == 56
            assert config.include_detailed_activities == True
            
            # Test data extraction (with mock credentials)
            garmin_data = await extract_garmin_data(
                email="test@example.com",
                password="test_password", 
                config=config
            )
            
            # Verify data structure
            assert isinstance(garmin_data, GarminData)
            assert garmin_data.user_profile is not None
            assert len(garmin_data.activities) > 0
            assert len(garmin_data.physiological_markers) > 0
            
            # Test data quality
            first_activity = garmin_data.activities[0]
            assert hasattr(first_activity, 'activity_name')
            assert hasattr(first_activity, 'sport_type')
            assert hasattr(first_activity, 'start_time')
            
            logger.info(f"‚úÖ Extracted {len(garmin_data.activities)} activities and {len(garmin_data.physiological_markers)} physiological markers")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Garmin data extraction test failed: {e}")
            return False
    
    async def test_individual_ai_agents(self) -> bool:
        """Test individual AI agent nodes."""
        
        logger.info("Testing individual AI agent nodes...")
        
        try:
            from app.services.ai.langgraph.state.training_analysis_state import create_initial_state
            from app.services.ai.langgraph.nodes.metrics_summarizer_node import metrics_summarizer_node
            from app.services.ai.langgraph.nodes.physiology_summarizer_node import physiology_summarizer_node
            from app.services.ai.langgraph.nodes.activity_summarizer_node import activity_summarizer_node
            
            # Create test state with mock data
            test_state = create_initial_state(
                analysis_id=str(uuid.uuid4()),
                user_id=str(uuid.uuid4()),
                training_config_id=str(uuid.uuid4()),
                analysis_config={"analysis_type": "full_analysis"}
            )
            
            # Add mock Garmin data
            test_state["garmin_data"] = {
                "activities": [
                    {
                        "activity_name": "Morning Run",
                        "sport_type": "running",
                        "start_time": "2024-01-10T08:00:00",
                        "moving_time": 3600,
                        "distance": 10000,
                        "average_heart_rate": 150
                    },
                    {
                        "activity_name": "Bike Workout", 
                        "sport_type": "cycling",
                        "start_time": "2024-01-11T09:00:00",
                        "moving_time": 5400,
                        "distance": 40000,
                        "average_power": 220
                    }
                ],
                "physiological_markers": [
                    {
                        "date": "2024-01-10",
                        "resting_heart_rate": 45,
                        "hrv_score": 85,
                        "sleep_score": 78,
                        "stress_level": "low"
                    }
                ]
            }
            
            # Test metrics summarizer
            metrics_result = await metrics_summarizer_node(test_state)
            assert "metrics_summary" in metrics_result
            assert metrics_result["metrics_summary"] is not None
            logger.info("‚úÖ Metrics summarizer working")
            
            # Test physiology summarizer  
            physiology_result = await physiology_summarizer_node(metrics_result)
            assert "physiology_summary" in physiology_result
            assert physiology_result["physiology_summary"] is not None
            logger.info("‚úÖ Physiology summarizer working")
            
            # Test activity summarizer
            activity_result = await activity_summarizer_node(physiology_result)
            assert "activity_summary" in activity_result
            assert activity_result["activity_summary"] is not None
            logger.info("‚úÖ Activity summarizer working")
            
            # Check token usage tracking
            assert "token_usage" in activity_result
            token_agents = activity_result["token_usage"]
            assert len(token_agents) >= 3  # At least 3 agents should have recorded usage
            
            logger.info("‚úÖ All AI agent nodes working correctly")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå AI agents test failed: {e}")
            return False
    
    async def test_workflow_orchestration(self) -> bool:
        """Test the LangGraph workflow orchestration."""
        
        logger.info("Testing workflow orchestration...")
        
        try:
            from app.services.ai.langgraph.workflows.training_analysis_workflow import workflow_engine
            from app.services.ai.langgraph.state.training_analysis_state import create_initial_state
            
            # Create test state
            test_state = create_initial_state(
                analysis_id=str(uuid.uuid4()),
                user_id=str(uuid.uuid4()),
                training_config_id=str(uuid.uuid4()),
                analysis_config={
                    "analysis_type": "full_analysis",
                    "ai_mode": "development",
                    "activities_days": 21,
                    "metrics_days": 56
                }
            )
            
            # Test workflow graph construction
            assert workflow_engine.graph is not None
            logger.info("‚úÖ Workflow graph constructed successfully")
            
            # Note: Full workflow execution would require all missing agent nodes
            # For now, we validate the workflow structure exists and is properly configured
            
            logger.info("‚úÖ Workflow orchestration system working")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Workflow orchestration test failed: {e}")
            return False
    
    async def test_analysis_engine_integration(self) -> bool:
        """Test the complete analysis engine integration."""
        
        logger.info("Testing analysis engine integration...")
        
        try:
            from app.services.ai.analysis_engine import analysis_engine
            
            # Test engine initialization
            assert analysis_engine.workflow is not None
            
            # Test configuration loading (with mock data)
            test_config = {
                "analysis_type": "full_analysis",
                "ai_mode": "development",
                "activities_days": 21,
                "metrics_days": 56,
                "enable_plotting": False
            }
            
            # Test analysis status checking (mock)
            mock_analysis_id = str(uuid.uuid4())
            status = await analysis_engine.get_analysis_status(mock_analysis_id, self.mock_db)
            
            assert isinstance(status, dict)
            assert "analysis_id" in status
            assert "status" in status
            
            logger.info("‚úÖ Analysis engine integration working")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Analysis engine test failed: {e}")
            return False
    
    async def test_database_integration(self) -> bool:
        """Test database integration and storage."""
        
        logger.info("Testing database integration...")
        
        try:
            from app.services.database.analysis_service import analysis_service
            from app.database.models.analysis import Analysis, AnalysisResult
            
            # Test service exists and has required methods
            required_methods = [
                "create_analysis",
                "get_analysis_status", 
                "save_analysis_results",
                "mark_analysis_failed"
            ]
            
            for method in required_methods:
                assert hasattr(analysis_service, method), f"Method {method} missing from analysis_service"
            
            # Test model structure
            assert hasattr(Analysis, 'status')
            assert hasattr(Analysis, 'analysis_type')
            assert hasattr(Analysis, 'total_tokens')
            
            assert hasattr(AnalysisResult, 'node_name')
            assert hasattr(AnalysisResult, 'content')
            assert hasattr(AnalysisResult, 'result_type')
            
            logger.info("‚úÖ Database integration working")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Database integration test failed: {e}")
            return False
    
    async def test_error_handling_robustness(self) -> bool:
        """Test error handling and recovery mechanisms."""
        
        logger.info("Testing error handling robustness...")
        
        try:
            from app.services.ai.langgraph.state.training_analysis_state import (
                create_initial_state,
                add_error,
                add_warning,
                validate_state_integrity
            )
            
            # Create test state
            state = create_initial_state(
                analysis_id=str(uuid.uuid4()),
                user_id=str(uuid.uuid4()),
                training_config_id=str(uuid.uuid4()),
                analysis_config={}
            )
            
            # Test error tracking
            initial_error_count = len(state["errors"])
            state = add_error(state, "Test error")
            assert len(state["errors"]) == initial_error_count + 1
            assert state["progress"].error_count == 1
            
            # Test warning tracking
            initial_warning_count = len(state["warnings"])
            state = add_warning(state, "Test warning")
            assert len(state["warnings"]) == initial_warning_count + 1
            
            # Test state validation with errors
            validation_issues = validate_state_integrity(state)
            
            logger.info("‚úÖ Error handling mechanisms working")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error handling test failed: {e}")
            return False
    
    async def run_comprehensive_validation(self) -> Dict[str, bool]:
        """Run all validation tests."""
        
        tests = [
            ("State Management System", self.test_state_management_system),
            ("Model Configuration System", self.test_model_configuration_system),
            ("Garmin Data Extraction", self.test_garmin_data_extraction),
            ("Individual AI Agents", self.test_individual_ai_agents),
            ("Workflow Orchestration", self.test_workflow_orchestration),
            ("Analysis Engine Integration", self.test_analysis_engine_integration),
            ("Database Integration", self.test_database_integration),
            ("Error Handling Robustness", self.test_error_handling_robustness)
        ]
        
        results = {}
        
        for test_name, test_func in tests:
            logger.info(f"\n" + "="*60)
            logger.info(f"Running test: {test_name}")
            logger.info("="*60)
            
            try:
                result = await test_func()
                results[test_name] = result
                
            except Exception as e:
                logger.error(f"Test {test_name} crashed: {e}")
                results[test_name] = False
        
        return results


async def main():
    """Run comprehensive AI system validation."""
    
    logger.info("üîç Starting comprehensive AI system validation...")
    
    validator = CompleteSystemValidator()
    results = await validator.run_comprehensive_validation()
    
    # Print detailed summary
    logger.info(f"\n" + "="*60)
    logger.info("COMPREHENSIVE AI SYSTEM VALIDATION SUMMARY")
    logger.info("="*60)
    
    passed_tests = 0
    total_tests = len(results)
    
    for test_name, result in results.items():
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        logger.info(f"{test_name:<35} {status}")
        if result:
            passed_tests += 1
    
    logger.info("="*60)
    
    success_rate = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
    
    if passed_tests == total_tests:
        final_status = f"üéâ ALL TESTS PASSED! ({passed_tests}/{total_tests}) - {success_rate:.0f}%"
    elif passed_tests > total_tests * 0.8:
        final_status = f"üü° MOSTLY SUCCESSFUL ({passed_tests}/{total_tests}) - {success_rate:.0f}%"
    else:
        final_status = f"üî¥ SIGNIFICANT ISSUES ({passed_tests}/{total_tests}) - {success_rate:.0f}%"
    
    logger.info(final_status)
    
    # Summary of system readiness
    logger.info("\nüîß SYSTEM READINESS ASSESSMENT:")
    
    critical_tests = [
        "State Management System",
        "Model Configuration System", 
        "Garmin Data Extraction",
        "Individual AI Agents"
    ]
    
    critical_passed = sum(1 for test in critical_tests if results.get(test, False))
    
    if critical_passed == len(critical_tests):
        logger.info("‚úÖ Core AI system components are fully functional")
        logger.info("‚úÖ System ready for end-to-end analysis workflows")
        logger.info("‚úÖ CLI AI behavior successfully replicated")
    else:
        logger.info("‚ö†Ô∏è Some core components need attention before production use")
    
    return passed_tests == total_tests


if __name__ == "__main__":
    # Run the comprehensive validation
    success = asyncio.run(main())
    exit(0 if success else 1)